使用词嵌入

将单词与向量相关联还有另一种常用的强大方法，就是使用密集的词向量（word vector），也叫词嵌入（word embedding）。 
one-hot 编码得到的向量是二进制的、稀疏的（绝大部分元素都是 0）、维度很高的（维度大小等于词表中的单词个数），
而词嵌入是低维的浮点数向量（即密集向量，与稀疏向量相对）。与 one-hot 编码得到的词向量不同，词嵌入是从数据中学习得到的。
常见的词向量维度是 256、 512 或 1024（处理非常大的词表时）。与此相对， onehot 编码的词向量维度通常为 20 000 或更高（对应包含 20 000 个标记的词表）。

1. 利用 Embedding 层学习词嵌入
词向量之间的几何关系应该表示这些词之间的语义关系。词嵌入的作用应该是将人类的语言映射到几何空间中。
对每个新任务都学习一个新的嵌入空间。幸运的是，反向传播让这种学习变得很简单，而 Keras 使其变得更简单。
我们要做的就是学习一个层的权重，这个层就是Embedding 层。

2. 使用预训练的词嵌入   
没有足够的数据来自己学习真正强大的特征，但你需要的特征应该是非常通用的，比如常见的视觉特征或语义特征。
在这种情况下，重复使用在其他问题上学到的特征。
word2vec
GloVe
