
分词：把长句子拆成有“意义”的小部件
 tokens = nltk.word_tokenize(sentence)
 中英文NLP区别：分词
 分词：a.启发式Heuristic   查字典的方式
      b.机器学习统计方法：HMM、CRF、NN(神经网络)
      
中文分词
   jieba
   全模式
   精确模式（默认）
   新词识别
   搜索引擎模式
   
有时候，分词没有那么简单
⽐如社交⽹络上，这些乱七⼋糟的不合语法不合正常逻辑的语⾔很多：
拯救 @某⼈, 表情符号, URL, #话题符号
可以用正则表达式处理
http://www.regexlab.com/zh/regref.htm 正则表达式对照表

tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)
emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)
def tokenize(s):
return tokens_re.findall(s)
def preprocess(s, lowercase=False):
tokens = tokenize(s)
if lowercase:
tokens = [token if emoticon_re.search(token) else token.lower() for token in
tokens]
return tokens
tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'
print(preprocess(tweet))
# ['RT', '@angelababy', ':', 'love', 'you', 'baby',
# ’!', ':D', 'http://ah.love', '#168cm']
