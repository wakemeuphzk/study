激活函数

使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。

激活函数：
1>sigmoid函数
2>tanh函数：是sigmoid的向下平移和伸缩后的结果。对它进行了变形后，穿过了点，并且值域介于+1和-1之间。
隐藏层上使用函数tanh函数效果总是优于sigmoid函数。因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。
在训练一个算法模型时，如果使用tanh函数代替sigmoid函数中心化数据，使得数据的平均值更接近0而不是0.5.
tanh函数在所有场合都优于sigmoid函数。
但有一个例外：在二分类的问题中，对于输出层，因为的值是0或1，所以想让的数值介于0和1之间，而不是在-1和+1之间。
所以需要使用sigmoid激活函数。在例子里看到的是，对隐藏层使用tanh激活函数，输出层使用sigmoid函数。

sigmoid函数和tanh函数两者共同的缺点是：
在特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。

3>修正线性单元的函数（ReLu）: a= max(0,z),只要是正值的情况下，导数恒等于1，当是负值的时候，导数恒等于0

选择激活函数的经验法则：
如果输出是0、1值（二分类问题），则输出层选择sigmoid函数，然后其它的所有单元都选择Relu函数。
这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用Relu激活函数。

4>Leaky Relu(带泄漏的ReLU): a= max(0.01z,z)当是负值时，这个函数的值不是等于0，而是轻微的倾斜
这个函数通常比Relu激活函数效果要好，尽管在实际中Leaky ReLu使用的并不多。

Relu和Leaky Relu两者的优点是：
第一，在的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个if-else语句，而sigmoid函数需要进行浮点四则运算，
在实践中，使用ReLu激活函数神经网络通常会比使用sigmoid或者tanh激活函数学习的更快。
第二，sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，
而Relu和Leaky ReLu函数大于0部分都为常数，不会产生梯度弥散现象。
(同时应该注意到的是，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而Leaky ReLu不会有这问题)

建议：
如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。
