随机初始化

对于逻辑回归，把权重初始化为0当然也是可以的。
但是对于一个神经网络，如果你把权重或者参数W都初始化为0，那么梯度下降将不会起作用。

如果你要初始化w成0，由于所有的隐含单元都是对称的，无论你运行梯度下降多久，他们一直计算同样的函数。
这没有任何帮助，因为你想要两个不同的隐含单元计算不同的函数，这个问题的解决方法就是随机初始化参数。

W设为np.random.randn(2,2)(生成高斯分布)，通常再乘上一个小的数，比如0.01，这样把它初始化为很小的随机数。
然后b没有这个对称的问题（叫做symmetry breaking problem），所以可以把b初始化为0
，因为只要随机初始化你就有不同的隐含单元计算不同的东西，因此不会有symmetry breaking问题了。

如果W很大，那么你很可能最终停在（甚至在训练刚刚开始的时候）z很大的值，这会造成tanh/Sigmoid激活函数饱和在龟速的学习上，
如果你没有sigmoid/tanh激活函数在你整个的神经网络里，就不成问题。但如果你做二分类并且你的输出单元是Sigmoid函数，
那么你不会想让初始参数太大，因此这就是为什么乘上0.01或者其他一些小数是合理的尝试。

事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。
但当你训练一个非常非常深的神经网络，你可能要试试0.01以外的常数。
