Batch Norm 为什么奏效？

一个原因是，你已经看到如何归一化输入特征值（但不仅仅对于这里的输入值，还有隐藏单元的值），使其均值为0，方差1，
它又是怎样加速学习的，有一些从0到1而不是从1到1000的特征值，通过归一化所有的输入特征值，以获得类似范围的值，可以加速学习。

另有一原因：
它可以使权重比你的网络更滞后或更深层，比如，第10层的权重更能经受得住变化，相比于神经网络中前层的权重，比如第1层。
“Covariate shift”：想法是这样的，如果你已经学习了x到y的映射，如果 x 的分布改变了，那么你可能需要重新训练你的学习算法。
Batch归一化做的，是它减少了这些隐藏值分布变化的数量。
Batch归一化减少了输入值改变的问题，它的确使这些值变得更稳定，神经网络的之后层就会有更坚实的基础。即使使输入分布改变了一些，它会改变得更少。
它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，你可以这样想，
它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。


batch Norm会带来轻微的正则化效果.
通过应用较大的min-batch，你减少了噪音，因此减少了正则化效果.
