为什么正则化有利于预防过拟合呢？

1.直观理解就是λ增加到足够大，W会接近于0，实际上是不会发生这种情况的，我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，
这个神经网络越来越接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，
实际上是该神经网络的所有隐藏单元依然存在，但是它们的影响变得更小了。

2.假设我们用的是双曲线激活函数tanh(z)。只要z非常小，如果只涉及少量参数，这里我们利用了双曲正切函数的线性状态
如果正则化参数λ很大，激活函数的参数会相对较小，因为代价函数中的参数变大了，
如果W很小，相对来说，z机会很小，g(z)=tanh(z)大致呈线性，每层几乎都是线性的，和线性回归函数一样。
如果每层都是线性的，那么整个网络就是一个线性网络，即使是一个非常深的深层网络，
因具有线性激活函数的特征，最终我们只能计算线性函数，因此，它不适用于非常复杂的决策.
