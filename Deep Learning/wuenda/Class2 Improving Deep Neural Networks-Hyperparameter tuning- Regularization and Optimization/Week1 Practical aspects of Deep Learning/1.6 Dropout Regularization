dropout 正则化

假设你在训练上图这样的神经网络，它存在过拟合，这就是dropout所要处理的，我们复制这个神经网络，dropout会遍历网络的每一层，
并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，
设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用backprop方法进行训练。
这是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。
对于每个训练样本，我们都将采用一个精简后神经网络来训练它。

如何实施dropout呢？
1.inverted dropout（反向随机失活）   最常用
看它是否小于某数，我们称之为keep-prob，keep-prob是一个具体数字，上个示例中它是0.5，
而本例中它是0.8，它表示保留某个隐藏单元的概率，此处keep-prob等于0.8，
它意味着消除任意一个隐藏单元的概率是0.2，它的作用就是生成随机矩阵。
事实证明，在测试阶段，当我们评估一个神经网络时，也就是用绿线框标注的反向随机失活方法，使测试阶段变得更容易，因为它的数据扩展问题变少

我们在测试阶段不使用dropout函数
Inverted dropout函数在除以keep-prob时可以记住上一步的操作，目的是确保即使在测试阶段不执行dropout来调整数值范围，
激活函数的预期结果也不会发生变化，所以没必要在测试阶段额外添加尺度参数，这与训练阶段不同。
