其他正则化方法

除了正则化和随机失活（dropout）正则化，还有几种方法可以减少神经网络中的过拟合:
1.数据扩增    人工合成数据，比如图片旋转、扭曲等
2.early stopping
提早停止训练神经网络
可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，
然后在某个节点处开始上升，early stopping的作用是，你会说，神经网络已经在这个迭代过程中表现得很好了，我们在此停止训练吧

当你还未在神经网络上运行太多迭代过程的时候，参数w接近0，因为随机初始化w值时，它的值可能都是较小的随机值，
所以在你长期训练神经网络之前w依然很小，在迭代过程和训练过程中w的值会变得越来越大，
比如在这儿，神经网络中参数w的值已经非常大了，所以early stopping要做就是在中间点停止迭代过程，我们得到一个值中等大小的弗罗贝尼乌斯范数，
与L2正则化相似，选择参数w范数较小的神经网络，但愿你的神经网络过度拟合不严重。

early stopping的主要缺点就是你不能独立地处理这两个问题:
a.选择一个算法来优化代价函数J,使其最小化
b.防止发生过拟合
因为提早停止梯度下降，也就是停止了优化代价函数J，因为现在你不再尝试降低代价函数J，所以代价函数的值可能不够小，同时你又希望不出现过拟合，
你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。

Early stopping的优点是，只运行一次梯度下降，你可以找出w的较小值，中间值和较大值，而无需尝试L2正则化超级参数的很多值。

正交化
