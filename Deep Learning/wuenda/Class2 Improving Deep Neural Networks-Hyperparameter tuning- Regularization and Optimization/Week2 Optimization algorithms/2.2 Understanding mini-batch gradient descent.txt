理解mini-batch梯度下降法

每次迭代下你都在训练不同的样本集或者说训练不同的mini-batch，如果你要作出成本函数的图，你很可能会看到这样的结果，走向朝下，但有更多的噪声.
你需要决定的变量之一是mini-batch的大小.

极端情况下，如果mini-batch的大小等于，其实就是batch梯度下降法:batch梯度下降法从某处开始，相对噪声低些，幅度也大一些，你可以继续找最小值。
另一个极端情况，假设mini-batch大小为1，就有了新的算法，叫做随机梯度下降法，每个样本都是独立的mini-batch:随机梯度下降法是有很多噪声的，
平均来看，它最终会靠近最小值，
不过有时候也会方向错误，因为随机梯度下降法永远不会收敛，而是会一直在最小值附近波动，但它并不会在达到最小值并停留在此。
会失去所有向量化带给你的加速,效率低下。

践中最好选择不大不小的mini-batch尺寸，实际上学习率达到最快。你会发现两个好处，
一方面，你得到了大量向量化，上个视频中我们用过的例子中，如果mini-batch大小为1000个样本，你就可以对1000个样本向量化，比你一次性处理多个样本快得多。
另一方面，你不需要等待整个训练集被处理完就可以开始进行后续工作，再用一下上个视频的数字，每次训练集允许我们采取5000个梯度下降步骤，
所以实际上一些位于中间的mini-batch大小效果最好。

指导原则：
1.如果训练集较小，直接使用batch梯度下降法，样本集较小就没必要使用mini-batch梯度下降法，你可以快速处理整个训练集，
所以使用batch梯度下降法也很好，这里的少是说小于2000个样本，这样比较适合使用batch梯度下降法。
2.样本数目较大的话，一般的mini-batch大小为64到512，考虑到电脑内存设置和使用的方式，如果mini-batch大小是2的n次方，代码会运行地快一些.
3.在你的mini-batch中，要确保符合CPU/GPU内存，取决于你的应用方向以及训练集的大小。
mini-batch大小是另一个重要的变量，你需要做一个快速尝试，才能找到能够最有效地减少成本函数的那个，我一般会尝试几个不同的值，几个不同的2次方，
然后看能否找到一个让梯度下降优化算法最高效的大小.
