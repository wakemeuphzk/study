使用SVM
不建议自己写代码求解参数，使用库中的，rg:liblinear,libsvm

需要自己做的事：
1.选择参数C
2.选择一个核函数kernel(相似函数)
     1>我们选择不需要任何内核参数，这种，也叫线性核函数（linear kernel）,比如在样本很少，但特征比较多的情况下，可以使用线性核函数
     2>高斯核函数，那么需要去选择对应的参数，这个参数要权衡方差与偏差,比如在样本数量中等，特征很少的情况下
     3>其他核函数（比较少使用）
           多项式核函数（Polynomial Kernel）：很少使用
           字符串核函数（String kernel）：当输入是文本字符串的时候
           卡方核函数（chi-square kernel）
           直方图交集核函数（histogram intersection kernel）

这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要满足 Mercer's 定理(核函数满足默赛尔定理)，才能被支持向量机的优化软件正确处理。
     
有些软件包可能需要你自己传入入参和出参

多分类问题：
很多包中已经内置了对应的API
如果一共有K个类，则我们需要K个模型，以及K个参数向量。我们同样也可以训练K个支持向量机来解决多类分类问题。

逻辑回归和SVM的比较：
当特征数量比样本个数多的情况下，可以选择逻辑回归或者带线性核函数的SVM;
当特征数量比较少，样本数量适中（成千上万），可以选择带高斯核函数的SVM;
当特征数量比较少，样本数量很多（5w,或几十上百万），带高斯核函数的SVM会比较慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或者带线性核函数的SVM。

逻辑回归和带线性核函数的SVM比较类似

什么时候使用神经网络？
神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。

但是通常更加重要的是：你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面，
通常这些方面会比你使用逻辑回归还是 SVM 这方面更加重要。

     
