决策树和随机森林

小象《09决策树和随机森林》

CART(Classification And Regression Tree):既可以用作分类，也可以用作回归,这种决策树

树的深度越深，越容易过拟合（根节点算第零层）
代码中生成文件.dot文件，可以用GVEdit打开，可以看到树的样子，包含特征与边界值
此.dot文件可以生成对应的图片，eg:dot Tpng -o 1.png 1.dot
源数据集有4个特征，为了方便画图，我们随机取了两个特征进行训练预测并plot,然后共有6个图，每个图的准确率是不一样的，由此可以初步看出那些特征重要

决策树做分类：
DecisionTreeClassifier(criterion='entropy', max_depth=3)
决策树分类的边界明显是有锯齿的
随机森林：
RandomForestClassifier(n_estimators=200, criterion='entropy', max_depth=4)  #200棵树

决策树做回归的例子
DecisionTreeRegressor(criterion='mse',max_depth=10)

熵：信息的不确定性   H(X)
概率低的事件最后发生了，则其熵值应该大
条件熵:H(Y|X) = H(X,Y)-H(X)       (X,Y)发生所包含的熵，减去X单独发生包含的熵：在X发生的前提下，Y发生‘新’带来的熵
推导过程

决策树的实例（weka自带测试数据）
1.决策树不一定是二叉树
2.并不是所有特质对构建决策树都有用

根节点熵有一个值，跟概率相关；叶子节点，熵为0，因为不确定性是0，此时每个叶子节点中的实例都属于同一类
决策树的构建过程，是一个熵不断降低的过程     决策树是一种贪心算法，可以找到局部最优，但不一定是全局最优

决策树学习的生成算法：建立决策树的关键，即在当前状态下选择哪个属性作为分类依据。根据不同的目标函数，建立决策树主要有以下三种算法
1.ID3:Iterative Dichotomiser
2.C4.5
3.CART

