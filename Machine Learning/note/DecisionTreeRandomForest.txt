决策树和随机森林

小象《09决策树和随机森林》

CART(Classification And Regression Tree):既可以用作分类，也可以用作回归,这种决策树

树的深度越深，越容易过拟合（根节点算第零层）
代码中生成文件.dot文件，可以用GVEdit打开，可以看到树的样子，包含特征与边界值
此.dot文件可以生成对应的图片，eg:dot Tpng -o 1.png 1.dot
源数据集有4个特征，为了方便画图，我们随机取了两个特征进行训练预测并plot,然后共有6个图，每个图的准确率是不一样的，由此可以初步看出那些特征重要

决策树做分类：
DecisionTreeClassifier(criterion='entropy', max_depth=3)
决策树分类的边界明显是有锯齿的
随机森林：
RandomForestClassifier(n_estimators=200, criterion='entropy', max_depth=4)  #200棵树

决策树做回归的例子
DecisionTreeRegressor(criterion='mse',max_depth=10)

熵：信息的不确定性   H(X)
概率低的事件最后发生了，则其熵值应该大
条件熵:H(Y|X) = H(X,Y)-H(X)       (X,Y)发生所包含的熵，减去X单独发生包含的熵：在X发生的前提下，Y发生‘新’带来的熵
推导过程

决策树的实例（weka自带测试数据）
1.决策树不一定是二叉树
2.并不是所有特质对构建决策树都有用

根节点熵有一个值，跟概率相关；叶子节点，熵为0，因为不确定性是0，此时每个叶子节点中的实例都属于同一类
决策树的构建过程，是一个熵不断降低的过程     决策树是一种贪心算法，可以找到局部最优，但不一定是全局最优

决策树学习的生成算法：建立决策树的关键，即在当前状态下选择哪个属性作为分类依据。根据不同的目标函数，建立决策树主要有以下三种算法
1.ID3:Iterative Dichotomiser
2.C4.5
3.CART

结合文恩图理解
联合熵/互信息I(X,Y)=H(X) + H(Y) - H(X,Y) = H(Y) - H(Y|X)
信息增益：当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别经验熵和经验条件熵。
信息增益表示得知特征A的信息而使得类X的信息的不确定性减少的程度,因此增益越大，表示减少越快，在构建决策树的时候，选择增益最大的特征
特征A对训练集D的信息增益g(D,A) = H(D) - H(D|A)
信息增益即互信息g(D,A) = H(D) - H(D|A) = I(A,D)

信息增益的问题：比如我把序号当作一个特征（取数多的属性），会发现其信息增益最大，但是这个字段毫无泛化能力，
               训练出来的是一颗庞大但深度浅的树，不合理   ID3
信息增益率   C4.5       Gr(D,A) = g(D,A)/H(A)
Gini系数第一定义:可看成-lnX的一阶近似   gini = 2p(1-p) 分类问题中，gini系数值越大越好，   CART
Gini系数第二定义：在经济学中经常用到，用来度量收入差距的指标，gini系数越大，表示收入差距越大，对于分类问题，gini越大越好
信息增益率或gini系数越大，表明属性对样本的熵减少的能力更强，这个属性使得数据由不确定性变成确定性的能力越强

对决策树的评价：
叶子结点才是导致最后结果的分类根据
叶子结点分为纯结点、均结点，其中纯结点最好，熵为0,最小；均结点效果最差，熵为lnK，最大
注意：如果所有都为纯结点，往往是过拟合

决策树的过拟合：
1.剪枝
2.随机森林

样本不均衡如何处理？




