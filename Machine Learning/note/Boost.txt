提升算法（Boost算法）
1.GDBT:梯度下降提升树
2.AdaBoost:自适应提升
3.XGBoost


小象学院《11.提升》

样本加权

提升：提升是一个机器学习技术，可以用于回归和分类问题，它每一步产生一个弱预测模型（eg决策树），并加权累加到总模型中；如果每一步的弱预测模型生成都是依赖损失函数的梯度方向，则称之为梯度提升（Gradient boosting）。
梯度提升：梯度提升算法首先给定一个目标损失函数，它的定义域是所有可行的弱函数集合（基函数）；提升算法通过迭代地选择一个负梯度方向上的基函数来逐渐逼近局部最小值。这种在函数域的梯度提升观点对机器学习的很多领域有深刻影响。
提升的理论意义：如果一个问题存在弱分类器，则可以通过提升的办法得到强分类器。

梯度提升方法寻找最优解F(x),使得损失函数在训练集上的期望最小。
中位数是绝对最小最优解

np.set_printoption(suppress=True)  :使输出不使用科学计数法（以正常小数方式显示）

GBDT(梯度提升决策树)
XGBoost:相对于传统的GBDT，XGBoost使用了二阶信息，可以更快地在训练集上收敛
Adaboost:对数据和分类器同时做调整
Adaboost模型是算法模型为加法模型、损失函数为指数函数、学习算法为前向分布算法时的二类学习方法。

boosting减少偏差，能够基于泛化能力较弱的分类器构造强学习器
bagging能够减少训练方差，对于不剪枝的决策树、神经网络等学习器有良好的集成效果

