小象《10 决策树和随机森林实践》

决策树很有防止过拟合的必要

1.预剪枝：在生成树的过程中就剪枝
2.后剪枝：先生成树，再剪枝      讨论的重点

剪枝的总体思路：
1.由完全树T0开始，剪枝部分节点得到T1，再剪枝部分节点得到T2,.....,直到仅剩树根的树TK;
2.在验证数据集上对这k个树分别进行评价，得到损失函数最小的树Ta(评价标准：信息增益、信息增益率、基尼系数)

叶子结点越大，决策树越复杂，越容易过拟合，损失越大，需要修正：对损失函数增加正则项

结点r的剪枝系数可以算出

剪枝算法：对于给定的决策树T0
1.计算所有内部结点的剪枝系数
2.查找最小剪枝系数的结点，剪枝得到决策树Tk
3.重复以上步骤，直到决策树Tk只剩一个结点
4.得到决策树序列T0 T1 T2 T3 ...... Tk
5.使用验证样本集选择最优子树

bootstraping  计算机是可以实现自举的

Bagging的策略：bootstrap aggregation
1.从样本集中重采样（有重复地）选出n个样本
2.在所有属性上，对这n个样本建立分类器（ID3、C4.5、CART、SVM、LR等）
3.重复以上两步m次，即获得了m个分类器
4.将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类
弱分类器(基本分类器)——————>强分类器
注意：SVM和LR本身就是强分类器

随机森林:随机森林在bagging基础上做了修改
1.从样本集中用bootstrap采样选出n个样本
2.在所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树       k：超参数
3.重复以上两步m次，即建立了m棵CART决策树
4.这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类

dropout有些像随机森林,为了防止过拟合
随机森林当初是为了对抗SVM产生的

随机森林也可以做回归

投票机制：
一般采用少数服从多数：有效多数（加权）
贝叶斯投票机制：为了防止一些冷门的得分低采取的方案

样本不均衡的常用处理方法：
1.不要总是用accuray指标，还可以用AUC、recall   
2.假设样本数目A类比B类多，且严重不平衡
   1>A类欠采样 Undersampling
       a.随机欠采样
       b.A类分成若干子类，分别与B类进入ML模型
       c.基于聚类的A类分割
   2>B类过采样 Oversampling
       避免欠采样造成的信息丢失
   3>B类数据合成 Synthetic Data Generation
       a.随机插值到新样本
       b.SMOTE(Synthetic Minority Over-sampling Technique)
   4>代价敏感学习 Cose Sensitive Learning
       降低A类权值，提高B类权值
   


