小象《10 决策树和随机森林实践》

决策树很有防止过拟合的必要

1.预剪枝：在生成树的过程中就剪枝
2.后剪枝：先生成树，再剪枝      讨论的重点

剪枝的总体思路：
1.由完全树T0开始，剪枝部分节点得到T1，再剪枝部分节点得到T2,.....,直到仅剩树根的树TK;
2.在验证数据集上对这k个树分别进行评价，得到损失函数最小的树Ta(评价标准：信息增益、信息增益率、基尼系数)

叶子结点越大，决策树越复杂，越容易过拟合，损失越大，需要修正：对损失函数增加正则项

结点r的剪枝系数可以算出

剪枝算法：对于给定的决策树T0
1.计算所有内部结点的剪枝系数
2.查找最小剪枝系数的结点，剪枝得到决策树Tk
3.重复以上步骤，直到决策树Tk只剩一个结点
4.得到决策树序列T0 T1 T2 T3 ...... Tk
5.使用验证样本集选择最优子树

bootstraping  计算机是可以实现自举的

Bagging的策略：bootstrap aggregation
1.从样本集中重采样（有重复地）选出n个样本
2.在所有属性上，对这n个样本建立分类器（ID3、C4.5、CART、SVM、LR等）
3.重复以上两步m次，即获得了m个分类器
4.将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类
弱分类器(基本分类器)——————>强分类器
注意：SVM和LR本身就是强分类器

随机森林
